{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5dEgRpy3952M"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcm\u001b[39;00m\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39muse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdark_background\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "## Load libraries\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "plt.style.use('dark_background')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4T7eUtw7Mh0z"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1e2N5S8MlCU"
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4EjOi-OM4Gp"
   },
   "outputs": [],
   "source": [
    "# Generate artificial data with 5 samples, 4 features per sample\n",
    "# and 3 output classes\n",
    "num_samples = 5 # number of samples\n",
    "num_features = 4 # number of features (a.k.a. dimensionality)\n",
    "num_labels = 3 # number of output labels\n",
    "# Data matrix (each column = single sample)\n",
    "X = np.random.choice(np.arange(3, 10), size = (num_features, num_samples), replace = True)\n",
    "# Class labels\n",
    "y = np.random.choice([0, 1, 2], size = num_samples, replace = True)\n",
    "print(X)\n",
    "print('------')\n",
    "print(y)\n",
    "print('------')\n",
    "# One-hot encode class labels\n",
    "y = tf.keras.utils.to_categorical(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrXipxwrJ0_8"
   },
   "source": [
    "---\n",
    "\n",
    "A generic layer class with forward and backward methods\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N4pKUhCyMrWm"
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "  def __init__(self):\n",
    "    self.input = None\n",
    "    self.output = None\n",
    "\n",
    "  def forward(self, input):\n",
    "    pass\n",
    "\n",
    "  def backward(self, output_gradient, learning_rate):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdLfiQSlOSUU"
   },
   "source": [
    "---\n",
    "\n",
    "The softmax classifier steps for a generic sample $\\mathbf{x}$ with (one-hot encoded) true label $\\mathbf{y}$ (3 possible categories) using a randomly initialized weights matrix (with bias abosrbed as its last last column):\n",
    "\n",
    "1. Calculate raw scores vector for a generic sample $\\mathbf{x}$  (bias feature added): $$\\mathbf{z} = \\mathbf{Wx}.$$\n",
    "2. Calculate softmax probabilities (that is, softmax-activate the raw scores) $$\\mathbf{a} = \\text{softmax}(\\mathbf{z})\\Rightarrow\\begin{bmatrix}a_0\\\\a_1\\\\a_2\\end{bmatrix}= \\text{softmax}\\left(\\begin{bmatrix}z_0\\\\z_1\\\\z_2\\end{bmatrix}\\right)=\\begin{bmatrix}\\frac{e^{z_0}}{e^{z_0}+e^{z_1}+e^{z_2}}\\\\\\frac{e^{z_1}}{e^{z_0}+e^{z_1}+e^{z_2}}\\\\\\frac{e^{z_2}}{e^{z_0}+e^{z_1}+e^{z_2}}\\end{bmatrix}$$\n",
    "3. Softmax loss for this sample is (where output label $y$ is not yet one-hot encoded)\n",
    "$$\\begin{align*}L &=  -\\log([\\mathbf{a}]_y) \\\\&= -\\log\\left(\\left[\\text{softmax}(\\mathbf{z})\\right]_y\\right)\\\\ &= -\\log\\left(\\left[\\text{softmax}(\\mathbf{Wx})\\right]_y\\right).\\end{align*}$$\n",
    "4. Predicted probability vector that the sample belongs to each one of the output categories is given a new name $$\\hat{\\mathbf{y}} = \\mathbf{a}.$$\n",
    "5. One-hot encoding the output label $$\\underbrace{y\\rightarrow\\mathbf{y}}_{\\text{e.g.}\\,2\\,\\rightarrow\\begin{bmatrix}0\\\\0\\\\1\\end{bmatrix}}$$ results in the following representation for the softmax loss for the sample which is also referred to as the categorical crossentropy (CCE) loss:\n",
    "$$\\begin{align*}L &= L\\left(\\mathbf{y},\\hat{\\mathbf{y}}\\right)=\\sum_{k=0}^2-y_k\\log\\left(\\hat{y}_k\\right)\\end{align*}.$$\n",
    "5. Calculate the gradient of the loss for the sample w.r.t. weights by following the computation graph from top to bottom (that is, backward):\n",
    "$$\\begin{align*} L\\\\{\\color{yellow}\\downarrow}\\\\ \\hat{\\mathbf{y}} &= \\mathbf{a}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{z}\\\\{\\color{yellow}\\downarrow}\\\\\\mathbf{W}\\end{align*}$$\n",
    "$$\\begin{align*}\\Rightarrow \\nabla_\\mathbf{W}(L) &= \\nabla_\\mathbf{W}(\\mathbf{z}) \\times\\nabla_\\mathbf{z}(\\hat{\\mathbf{y}})\\times\\nabla_{\\hat{\\mathbf{y}}}(L)\\\\&= \\underbrace{\\nabla_\\mathbf{W}(\\mathbf{z})}_\\text{first term} \\times\\underbrace{\\nabla_\\mathbf{z}(\\mathbf{a})}_\\text{second to last term}\\times\\underbrace{\\nabla_\\hat{\\mathbf{y}}(L)}_\\text{last term}.\\end{align*}$$\n",
    "7. Now focus on the last term $\\nabla_\\hat{\\mathbf{y}}(L)$:\n",
    "$$\\begin{align*}\\nabla_\\hat{\\mathbf{y}}(L) &=\\begin{bmatrix}\\nabla_{\\hat{y}_0}(L)\\\\\\nabla_{\\hat{y}_1}(L)\\\\\\nabla_{\\hat{y}_2}(L)\\end{bmatrix} = \\begin{bmatrix}-y_0/\\hat{y}_0\\\\-y_1/\\hat{y}_1\\\\-y_2/\\hat{y}_2\\end{bmatrix}.\\end{align*}$$\n",
    "8. Now focus on the second to last term $\\nabla_\\mathbf{z}(\\mathbf{a})$:\n",
    "$$\\begin{align*}\\nabla_\\mathbf{z}(\\mathbf{a}) &= \\nabla_\\mathbf{z}\\left(\\begin{bmatrix}a_0\\\\a_1\\\\a_2\\end{bmatrix}\\right)\\\\ &= \\begin{bmatrix}\\nabla_\\mathbf{z}(a_0)&\\nabla_\\mathbf{z}(a_1)&\\nabla_\\mathbf{z}(a_2)\\end{bmatrix} \\\\&= \\begin{bmatrix}\\nabla_{z_0}(a_0)&\\nabla_{z_0}(a_1)&\\nabla_{z_0}(a_2)\\\\\\nabla_{z_1}(a_0)&\\nabla_{z_1}(a_1)&\\nabla_{z_1}(a_2)\\\\\\nabla_{z_2}(a_0)&\\nabla_{z_2}(a_1)&\\nabla_{z_2}(a_2)\\end{bmatrix}\\\\&=\\begin{bmatrix}a_0(1-a_0)&-a_1a_0&-a_2a_0\\\\-a_0a_1&a_1(1-a_1)&-a_2a_1\\\\-a_0a_2&-a_1a_2&a_2(1-a_2)\\end{bmatrix}.\\end{align*}$$\n",
    "9. During the webinar, we will focus on the first term to complete the gradient calculation using the computation graph.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9YGwzbz72CZ"
   },
   "source": [
    "---\n",
    "\n",
    "Loss and its gradient\n",
    "\n",
    "$$\\begin{align*}L &= L\\left(\\mathbf{y},\\hat{\\mathbf{y}}\\right)=\\sum_{k=0}^2-y_k\\log\\left(\\hat{y}_k\\right)\\\\\\nabla_\\hat{\\mathbf{y}}(L) &=\\begin{bmatrix}\\nabla_{\\hat{y}_0}(L)\\\\\\nabla_{\\hat{y}_1}(L)\\\\\\nabla_{\\hat{y}_2}(L)\\end{bmatrix} = \\begin{bmatrix}-y_0/\\hat{y}_0\\\\-y_1/\\hat{y}_1\\\\-y_2/\\hat{y}_2\\end{bmatrix}.\\end{align*}$$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hdXSGW2s7zKd"
   },
   "outputs": [],
   "source": [
    "## Define the loss function and its gradient\n",
    "def cce(y, yhat):\n",
    "  return(-np.sum(y*np.log(yhat)))\n",
    "\n",
    "def cce_gradient(y, yhat):\n",
    "  return(-y/yhat)\n",
    "\n",
    "# TensorFlow in-built function for categorical crossentropy loss\n",
    "#cce = tf.keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smgXLg9p65HV"
   },
   "source": [
    "---\n",
    "\n",
    "Softmax activation layer class\n",
    "\n",
    "$$\\begin{align*}\\nabla_\\mathbf{z}(L) &= \\nabla_{\\mathbf{z}}(\\mathbf{a})\\times\\nabla_{\\mathbf{a}}(L)\\\\&=\\begin{bmatrix}a_0(1-a_0)&-a_1a_0&-a_2a_0\\\\-a_0a_1&a_1(1-a_1)&-a_2a_1\\\\-a_0a_2&-a_1a_2&a_2(1-a_2)\\end{bmatrix}\\begin{bmatrix}-y_0/\\hat{y}_0\\\\-y_1/\\hat{y}_1\\\\-y_2/\\hat{y}_2\\end{bmatrix}.\\end{align*}$$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4x1Xn3AbJlNy"
   },
   "outputs": [],
   "source": [
    "## Softmax activation layer class\n",
    "class Softmax(Layer):\n",
    "  def forward(self, input):\n",
    "    self.output = tf.nn.softmax(input).numpy()\n",
    "\n",
    "  def backward(self, output_gradient, learning_rate = None):\n",
    "    return(np.dot((np.identity(np.size(self.output))-self.output.T) * self.output, output_gradient))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkPFfd1U68dj"
   },
   "source": [
    "---\n",
    "\n",
    "Dense layer class\n",
    "\n",
    "$$\\begin{align*}\\nabla_\\mathbf{W}(L)&=\\nabla_{\\mathbf{W}}(\\mathbf{z})\\times\\nabla_{\\mathbf{z}}(L)\\\\&=\\nabla_{\\mathbf{z}}(L)\\mathbf{x}^\\mathrm{T}.\\end{align*}$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ctXhZYCTmHK"
   },
   "outputs": [],
   "source": [
    "## Dense layer class\n",
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(output_size, input_size+1) # bias trick\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = np.append(input, 1) # bias trick\n",
    "        self.output= np.dot(self.weights, self.input)\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        weights_gradient = np.dot(output_gradient.reshape(-1, 1), self.input.reshape(-1, 1).T)\n",
    "        input_gradient = np.dot(self.weights.T, output_gradient)\n",
    "        self.weights -= learning_rate * weights_gradient\n",
    "        return input_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NPVu8RC5Tvmy"
   },
   "outputs": [],
   "source": [
    "# Forward and backward propagation for the 0th sample\n",
    "learning_rate = 1e-02 # learning rate\n",
    "dlayer = Dense(num_features, num_labels) # define dense layer\n",
    "print(dlayer.weights)\n",
    "dlayer.forward(X[:, 0]) # forward prop\n",
    "softmax = Softmax() # define softmax activation\n",
    "softmax.forward(dlayer.output) # Softmax activate\n",
    "loss = cce(y[0, :], softmax.output)\n",
    "\n",
    "grad = output_gradient = cce_gradient(y[0, :], softmax.output)\n",
    "grad = softmax.backward(grad)\n",
    "grad = dlayer.backward(grad, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGIzrN-rPuI4"
   },
   "outputs": [],
   "source": [
    "## Train the 0-layer neural network using batch training with batch size = 1\n",
    "\n",
    "# Steps: run over each sample, calculate loss, gradient of loss,\n",
    "# and update weights.\n",
    "\n",
    "learning_rate = 1e-02 # learning rate\n",
    "loss = 0 # initialize loss\n",
    "dlayer = Dense(num_features, num_labels) # define dense layer\n",
    "softmax = Softmax() # define softmax activation layer\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "  dlayer.forward(X[:, i]) # forward prop\n",
    "  softmax.forward(dlayer.output) # Softmax activate\n",
    "  loss += cce(y[i, :], softmax.output) # calculate loss for sample\n",
    "  # Backward prop starts here\n",
    "  grad = cce_gradient(y[i, :], softmax.output)\n",
    "  grad = softmax.backward(output_gradient = grad)\n",
    "  grad = dlayer.backward(grad, learning_rate)\n",
    "\n",
    "print(loss/X.shape[1])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
